 Iris Dataset Poisoning Experiment â€” MLflow Tracked Pipeline
This project performs a controlled data poisoning experiment on the Iris dataset to measure how ML model performance degrades under various types and severities of poisoning.
Both feature-noise poisoning and label-flip poisoning are introduced at multiple fractions, and the entire training pipeline is tracked using MLflow.

ðŸ§ª 1. Objective
The goal of this experiment is to:


Demonstrate how data poisoning affects model performance


Compare feature corruption vs label corruption


Track all runs using MLflow experiments


Study how accuracy and metrics decay at poison levels: 5%, 10%, 50%


Understand defenses and how data quantity requirements change when quality is degraded



ðŸ“‚ 2. Project Structure
bashCopy codeâ”œâ”€â”€ train.py                    # Main experiment & poisoning pipeline
â”œâ”€â”€ test.py                     # MLflow-based sanity test (loads latest model)
â”œâ”€â”€ data_iris/
â”‚   â””â”€â”€ iris.csv                # Dataset
â”œâ”€â”€ poison_experiments/
â”‚   â”œâ”€â”€ model_*.joblib          # Models saved locally
â”‚   â”œâ”€â”€ poison_results_summary.csv
â”‚   â””â”€â”€ accuracy_vs_poison.png
â”œâ”€â”€ mlruns/                     # MLflow experiment logs
â””â”€â”€ README.md


ðŸ§© 3. Poisoning Types Implemented
âœ” A. Feature-Noise Poisoning
Random numbers replace feature values for a subset of rows.
arduinoCopy codesepal_length â†’ random value within minâ€“max range
sepal_width  â†’ random value within minâ€“max range

Fraction poisoned:
5%, 10%, 50%

âœ” B. Label-Flip Poisoning
Correct labels are replaced with a random incorrect class.
Example:
nginxCopy codesetosa â†’ versicolor
versicolor â†’ virginica

Fraction poisoned:
5%, 10%, 50%

âš™ï¸ 4. Training Process
Each run performs:


Optional poisoning of the dataset


Train/test split (stratified)


Train a DecisionTreeClassifier(max_depth=3)


Log:


Poison type


Poison percentage


Accuracy, precision, recall, F1


Full sklearn model â†’ MLflow (with signature + input example)


Joblib model â†’ local artifact




MLflow Signature Logging
pythonCopy codesignature = infer_signature(X_train, model.predict(X_train))
mlflow.sklearn.log_model(
    model,
    name="model",
    input_example=X_train.iloc[:1],
    signature=signature
)


ðŸ“ 5. How to Run
Install dependencies:
bashCopy codepip install pandas scikit-learn joblib mlflow matplotlib

Run the poisoning experiment:
bashCopy codepython3 train.py

View MLflow UI:
bashCopy codemlflow ui --backend-store-uri mlruns

Open browser to:
cppCopy codehttp://127.0.0.1:5000


ðŸ” 6. Sanity-Test Model Loading (test.py)
This script:


Loads latest MLflow run


Loads model using mlflow.pyfunc.load_model()


Performs a prediction on a known Iris sample


Ensures signature consistency


Run:
bashCopy codepython3 test.py

Example output:
yamlCopy codeðŸ“Œ Using MLflow Experiment: iris_poisoning_experiment
ðŸ“Œ Latest Run ID: xxxxxx
ðŸ“Œ Loading model from mlflow...
âœ… Test 1/3: Model loaded
ðŸ“Œ Predicted: setosa
ðŸŽ‰ SUCCESS: All MLflow sanity checks passed!


ðŸ“Š 7. Output Artifacts
ðŸ“ poison_experiments/poison_results_summary.csv
Contains:
poison_typepoison_fractionaccuracyprecisionrecallf1
ðŸ“ˆ accuracy_vs_poison.png
Graph showing accuracy decay as poisoning increases.

ðŸ§  8. Expected Observations
âœ” Feature-Noise Poisoning


At 5%, model accuracy drops slightly


At 10%, noticeable performance degradation


At 50%, model becomes nearly unusable
Feature corruption makes input distribution unstable â†’ unpredictable splits â†’ poor generalization.



âœ” Label-Flip Poisoning


Much more harmful than feature noise


Even 5% label flips strongly reduce accuracy


50% flips â†’ model becomes random guesser
Label noise directly disrupts decision boundaries.



ðŸ›¡ï¸ 9. Mitigation Strategies for Poisoning Attacks
âœ” Data-Level Defenses


Outlier detection (Isolation Forest, LOF)


Clustering-based detection of "odd" samples


Statistical tests on feature distributions


Cross-dataset validation


Label verification using majority voting



âœ” Model-Level Defenses


Robust loss functions (Huber, Savage, MAE)


Ensemble methods which resist mislabeled points


Differential privacy models (limits per-sample effect)


Trimmed mean or Krum aggregation (for FL settings)



âœ” Pipeline-Level Defenses


Automated data profiling in CI/CD


Drift detection between training data versions


Shadow models to compare poisoning impact


Data versioning with DVC or DeltaLake



ðŸ“ˆ 10. Data Quantity Requirements Under Poisoning
When data quality degrades:
Poison LevelEffectHow Much More Data Needed?5%Model tolerates+30â€“40% more clean samples10%Decision boundaries shift+1.5â€“2Ã— dataset size50%Training becomes unreliableData must be cleaned, not increased
Key insight:
Bad data cannot be compensated by more data. Cleaning > collecting.

ðŸ§© 11. Conclusion
This experiment clearly shows:


MLflow enables full tracking of poisoning experiments


Even low-level poisoning severely affects classical ML models


Label poisoning is more dangerous than feature poisoning


Data validation, model robustness, and clean pipelines are essential


This repository provides a complete reference implementation of:
âœ” Poison data generation
âœ” ML training with artifacts
âœ” MLflow experiment management
âœ” Model validation and CI automation
